{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe38b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataset in ./opt/anaconda3/lib/python3.8/site-packages (1.5.2)\n",
      "Requirement already satisfied: banal>=1.0.1 in ./opt/anaconda3/lib/python3.8/site-packages (from dataset) (1.0.6)\n",
      "Requirement already satisfied: alembic>=0.6.2 in ./opt/anaconda3/lib/python3.8/site-packages (from dataset) (1.7.7)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.2 in ./opt/anaconda3/lib/python3.8/site-packages (from dataset) (1.4.36)\n",
      "Requirement already satisfied: importlib-metadata in ./opt/anaconda3/lib/python3.8/site-packages (from alembic>=0.6.2->dataset) (3.10.0)\n",
      "Requirement already satisfied: importlib-resources in ./opt/anaconda3/lib/python3.8/site-packages (from alembic>=0.6.2->dataset) (5.7.1)\n",
      "Requirement already satisfied: Mako in ./opt/anaconda3/lib/python3.8/site-packages (from alembic>=0.6.2->dataset) (1.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./opt/anaconda3/lib/python3.8/site-packages (from sqlalchemy>=1.3.2->dataset) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata->alembic>=0.6.2->dataset) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./opt/anaconda3/lib/python3.8/site-packages (from Mako->alembic>=0.6.2->dataset) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset\n",
    "import requests\n",
    "import dataset\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d452fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now... http://quotes.toscrape.com/\n",
      "Now... http://quotes.toscrape.com/page/2/\n",
      "Now... http://quotes.toscrape.com/page/3/\n",
      "Now... http://quotes.toscrape.com/page/4/\n",
      "Now... http://quotes.toscrape.com/page/5/\n",
      "Now... http://quotes.toscrape.com/page/6/\n",
      "Now... http://quotes.toscrape.com/page/7/\n",
      "Now... http://quotes.toscrape.com/page/8/\n",
      "Now... http://quotes.toscrape.com/page/9/\n",
      "Now... http://quotes.toscrape.com/page/10/\n",
      "Now... http://quotes.toscrape.com/author/author\n"
     ]
    }
   ],
   "source": [
    "db =  dataset.connect('sqlite:///quotes.db')\n",
    "\n",
    "authors_seen = set()  #空集合\n",
    "\n",
    "base_url = 'http://quotes.toscrape.com/'\n",
    "\n",
    "def clean_url(url):\n",
    "        #'/author/Steve-Martin'を'Steve-Martinに変更してCleanにする\n",
    "        #まず、urljoinを使って絶対URLにする\n",
    "        url = urljoin(base_url, str(url))\n",
    "        #urlparseを使ってpathの部分を取り出す\n",
    "        path = urlparse(url).path\n",
    "        #'/author/以下がpathとして出てくる'\n",
    "        \n",
    "        #'/'でpathを分割、２番目の値を取得\n",
    "        return path.split('/')[1]\n",
    "\n",
    "def scrape_quotes(html_soup):\n",
    "        quotes = html_soup.select('div.quote')\n",
    "        for quote in quotes:\n",
    "                quote_text = quote.find(class_ = 'text').get_text(strip = True)\n",
    "                quote_author_url = clean_url(quote.find(class_ = 'author').find_next_sibling('a').get('href'))\n",
    "                quote_tag_urls = [clean_url(a.get('href') for a in quote.find_all('a', class_ = 'tag'))]\n",
    "                authors_seen.add(quote_author_url)\n",
    "                \n",
    "                #この名言とタグを保存する\n",
    "                quote_id = db['quotes'].insert(\n",
    "                    {'text': quote_text, 'author': quote_author_url})\n",
    "                db['quote_tags'].insert_many(\n",
    "                [{'quote_id': quote_id, 'tag_id': tag} for tag in quote_tag_urls])\n",
    "\n",
    "def scrape_authors(html_soup, author_id):\n",
    "        author_name = html_soup.find(class_ = 'author-title').get_text(strip = True)\n",
    "        author_born_date = html_soup.find(class_ = 'author-born-date').get_text(strip = True)\n",
    "        author_born_loc = html_soup.find(class_ = 'author-born-location').get_text(strip = True)\n",
    "        author_discription = html_soup.find(class_ = 'author-description').get_text(strip = True)\n",
    "        db['authors'].insert({\n",
    "            'author_id': author_id, 'name': author_name, 'born_date': author_born_date, 'born_location': author_born_loc, 'description': author_discription\n",
    "        })\n",
    "\n",
    "#まず全てのページの名言をスクレイピングする\n",
    "url = base_url\n",
    "while True:\n",
    "        print('Now...', url)\n",
    "        res = requests.get(url)\n",
    "        html_soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        #名言をスクレイピング\n",
    "        scrape_quotes(html_soup)\n",
    "        \n",
    "        #次のページがあるか\n",
    "        next_a = html_soup.select('li.next > a')\n",
    "        if not next_a or not next_a[0].get('href'):\n",
    "                break\n",
    "        url = urljoin(url, next_a[0].get('href'))\n",
    "        \n",
    "#発言者情報を取り出す\n",
    "for author_id in authors_seen:\n",
    "        url = urljoin(base_url, '/author/' + author_id)\n",
    "        print('Now...', url)\n",
    "        res = requests.get(url)\n",
    "        html_soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        \n",
    "        #発言者情報を拾う\n",
    "        scrape_authors(html_soup, author_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892e9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
